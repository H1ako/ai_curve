{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NikaR\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dataset = load_from_disk(\"./twitch_chats\")\n",
    "dataset_data = dataset['train']['json'][0][0]['chat']\n",
    "data = \"\"\n",
    "for i in dataset_data:\n",
    "    data += i['message'] + \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 184\n"
     ]
    }
   ],
   "source": [
    "# chars = sorted(list(set(data)))\n",
    "words = sorted(list(set(data.split())))\n",
    "\n",
    "vocab_size = len(words)\n",
    "# vocab_size = len(chars)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(words)}\n",
    "itos = {i: ch for i, ch in enumerate(words)}\n",
    "# stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "# itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# for index in stoi:\n",
    "\n",
    "encode = lambda x: [stoi[ch] for ch in x.split()]\n",
    "# encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda l : ' '.join([itos[i] for i in l])\n",
    "\n",
    "data = encode(data)\n",
    "\n",
    "data = torch.tensor(data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    " # device = 'cuda'\n",
    "\n",
    "# n_embed = 32\n",
    "# num_heads = 4\n",
    "# n_layers = 3\n",
    "n_embed = 32\n",
    "num_heads = 4\n",
    "max_iters = 5000\n",
    "n_layers = 4\n",
    "dropout = 0.2\n",
    "\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "\n",
    "total_len = len(data)\n",
    "train_data = data[: int(total_len * 0.8)]\n",
    "test_data = data[int(total_len * 0.8) :]\n",
    "\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "# batch_size = 32\n",
    "# block_size = 128\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(data[i : i + block_size]) for i in ix])\n",
    "    y = torch.stack([torch.tensor(data[i + 1 : i + block_size + 1]) for i in ix])\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(184, 32)\n",
       "  (position_emabedding_table): Embedding(32, 32)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=32, out_features=184, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models\n",
    "\n",
    "m = models.BigramLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embed=n_embed,\n",
    "    block_size=block_size,\n",
    "    num_heads=num_heads,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NikaR\\AppData\\Local\\Temp\\ipykernel_29188\\3808155101.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(data[i : i + block_size]) for i in ix])\n",
      "C:\\Users\\NikaR\\AppData\\Local\\Temp\\ipykernel_29188\\3808155101.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(data[i + 1 : i + block_size + 1]) for i in ix])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train loss: 5.3880, test loss: 5.4806\n",
      "step 100, train loss: 2.4529, test loss: 5.7531\n",
      "step 200, train loss: 0.8442, test loss: 6.4895\n",
      "step 300, train loss: 0.3819, test loss: 7.2208\n",
      "step 400, train loss: 0.2263, test loss: 7.7186\n",
      "step 500, train loss: 0.1491, test loss: 8.1841\n",
      "step 600, train loss: 0.1146, test loss: 8.3690\n",
      "step 700, train loss: 0.0928, test loss: 8.8013\n",
      "step 800, train loss: 0.0753, test loss: 8.9806\n",
      "step 900, train loss: 0.0630, test loss: 9.3375\n",
      "step 1000, train loss: 0.0567, test loss: 9.4545\n",
      "step 1100, train loss: 0.0471, test loss: 9.6702\n",
      "step 1200, train loss: 0.0400, test loss: 9.7829\n",
      "step 1300, train loss: 0.0346, test loss: 9.9163\n",
      "step 1400, train loss: 0.0300, test loss: 10.1044\n",
      "step 1500, train loss: 0.0276, test loss: 10.2505\n",
      "step 1600, train loss: 0.0257, test loss: 10.3432\n",
      "step 1700, train loss: 0.0224, test loss: 10.5344\n",
      "step 1800, train loss: 0.0215, test loss: 10.5428\n",
      "step 1900, train loss: 0.0211, test loss: 10.6285\n",
      "step 2000, train loss: 0.0197, test loss: 10.8127\n",
      "step 2100, train loss: 0.0181, test loss: 10.9312\n",
      "step 2200, train loss: 0.0178, test loss: 10.9161\n",
      "step 2300, train loss: 0.0194, test loss: 11.0478\n",
      "step 2400, train loss: 0.0178, test loss: 11.2138\n",
      "step 2500, train loss: 0.0190, test loss: 11.3617\n",
      "step 2600, train loss: 0.0182, test loss: 11.4546\n",
      "step 2700, train loss: 0.0187, test loss: 11.5265\n",
      "step 2800, train loss: 0.0175, test loss: 11.6834\n",
      "step 2900, train loss: 0.0171, test loss: 11.5923\n",
      "step 3000, train loss: 0.0180, test loss: 11.7018\n",
      "step 3100, train loss: 0.0186, test loss: 11.8585\n",
      "step 3200, train loss: 0.0177, test loss: 11.7372\n",
      "step 3300, train loss: 0.0167, test loss: 11.8387\n",
      "step 3400, train loss: 0.0175, test loss: 11.9905\n",
      "step 3500, train loss: 0.0170, test loss: 12.0126\n",
      "step 3600, train loss: 0.0168, test loss: 12.1454\n",
      "step 3700, train loss: 0.0170, test loss: 12.0907\n",
      "step 3800, train loss: 0.0181, test loss: 12.0497\n",
      "step 3900, train loss: 0.0175, test loss: 12.3025\n",
      "step 4000, train loss: 0.0160, test loss: 12.2060\n",
      "step 4100, train loss: 0.0161, test loss: 12.3289\n",
      "step 4200, train loss: 0.0169, test loss: 12.3118\n",
      "step 4300, train loss: 0.0168, test loss: 12.5228\n",
      "step 4400, train loss: 0.0164, test loss: 12.5931\n",
      "step 4500, train loss: 0.0155, test loss: 12.5697\n",
      "step 4600, train loss: 0.0152, test loss: 12.6048\n",
      "step 4700, train loss: 0.0167, test loss: 12.6009\n",
      "step 4800, train loss: 0.0174, test loss: 12.6031\n",
      "step 4900, train loss: 0.0165, test loss: 12.6618\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = m(x, y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "    m.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "for step in range(max_iters):\n",
    "    x, y = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = m(x, y)\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {step}, train loss: {losses['train']:.4f}, test loss: {losses['test']:.4f}\"\n",
    "        )\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NikaR\\AppData\\Local\\Temp\\ipykernel_29188\\3808155101.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(data[i : i + block_size]) for i in ix])\n",
      "C:\\Users\\NikaR\\AppData\\Local\\Temp\\ipykernel_29188\\3808155101.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(data[i + 1 : i + block_size + 1]) for i in ix])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: dumb fuck we on saturn Take 2 years to cook after release lucypySadge LUL atpRtsd Allows you to flush it before others get their copies Devs going to hell. 👏 -100 atpFeelsBeardMan\n",
      "output: dumb fuck we on saturn Take 2 years to cook after release lucypySadge LUL atpRtsd Allows you to flush it before others get their copies Devs going to hell. 👏 -100 atpFeelsBeardMan 100 LMAO @AvoidingThePuddle game is free with wow gold though. atpWind atpGasm atpSpiner atpRtsd LUL LUL atpRtsd LUL LUL ugh they got me with that shit taticadsESTINHUS taticadsBUIUIA they going to hell\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch('test')\n",
    "\n",
    "out = m.generate(x, max_new_tokens=block_size)\n",
    "\n",
    "print(\"input:\", decode(x[0].tolist()))\n",
    "print(\"output:\", decode(out[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
